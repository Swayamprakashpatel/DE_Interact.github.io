{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swayamprakashpatel/DE_Interact.github.io/blob/main/Gen_AI_Drug_Discovery_SKP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HLF4bdAjxmc-",
      "metadata": {
        "id": "HLF4bdAjxmc-"
      },
      "source": [
        "# **Generative AI Model for Drug Discovery**\n",
        "\n",
        "\n",
        "**Dr. Swayamprakash Patel**\n",
        "Associate Professor,\n",
        "RPCP | CHARUSAT\n",
        "\n",
        "Email: Swayamprakash.patel@gmail.com\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bh_1uTxWMoD5",
      "metadata": {
        "cellView": "form",
        "id": "Bh_1uTxWMoD5"
      },
      "outputs": [],
      "source": [
        "# @title Model Download\n",
        "# Suppress all output from pip installation\n",
        "!pip install gdown > /dev/null 2>&1\n",
        "import gdown\n",
        "\n",
        "# CVAE Model download\n",
        "file_id = '17xSw68Rbfod9qxIZFAxrwgvbCyOFkll-'  # Replace with your actual Model file ID - CVAE Model\n",
        "destination = '/content/Gen_AI_Model.pt'  # Desired destination path\n",
        "\n",
        "# quiet=True suppresses the process printout\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', destination, quiet=True)\n",
        "\n",
        "# unique smile list download\n",
        "file_id2 = '18tmu6boTvDRcPQVURBUoBZCH_EqXwu-0'  # Replace with your actual file ID\n",
        "destination2 = '/content/unique_smiles_list.csv'  # Desired destination path\n",
        "\n",
        "# quiet=True suppresses the process printout\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id2}', destination2, quiet=True)\n",
        "\n",
        "# Print the final message after all downloads are complete\n",
        "print(\"SKP Model Downloaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OAt-jW7x_XCj",
      "metadata": {
        "id": "OAt-jW7x_XCj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Genrative_AI_Drug_Design\n",
        "#from google.colab import drive\n",
        "\n",
        "# --- Context Manager for Output Suppression ---\n",
        "# This is a standard Python technique to temporarily silence printing.\n",
        "class HiddenPrint:\n",
        "    \"\"\"Context manager to suppress stdout (print statements).\"\"\"\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        # Use os.devnull to redirect stdout to null\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "# --- SUPPRESS ALL SETUP OUTPUT (Installation and Loading Status) ---\n",
        "# Wrapping the entire setup in a HiddenPrint block and suppressing !pip output.\n",
        "\n",
        "#-------------------------------------------#\n",
        "#...........INSTALL LIBRARIESS..............#\n",
        "# Redirect stdout/stderr of pip install to /dev/null for maximum silence\n",
        "!pip install torch > /dev/null 2>&1\n",
        "!pip install transformers > /dev/null 2>&1\n",
        "!pip install numpy > /dev/null 2>&1\n",
        "!pip install rdkit > /dev/null 2>&1\n",
        "!pip install Pillow > /dev/null 2>&1\n",
        "!pip install transformers pillow > /dev/null 2>&1\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import warnings\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, EsmModel, AutoModel\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from PIL import Image\n",
        "from IPython.display import display # <-- CRITICAL FOR COLAB DISPLAY\n",
        "\n",
        "\n",
        "# Suppress warnings and parallel tokenizers for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. CONFIGURATION AND INITIALIZATION (SUPPRESSED)\n",
        "# ==============================================================================\n",
        "\n",
        "with HiddenPrint():\n",
        "    # --- File Paths and Device ---\n",
        "    # !!! IMPORTANT: UPDATE THIS PATH TO YOUR ACTUAL BEST MODEL FILE !!!\n",
        "    BEST_MODEL_PATH = '/content/Gen_AI_Model.pt' # from Direct Content\n",
        "\n",
        "    # ******************************************************************************\n",
        "    EMBEDDING_SAVE_PATH = 'protein_embeddings.npy'\n",
        "    SEQ_INDEX_PATH = 'protein_seq_index.json'\n",
        "    OUTPUT_DIR = 'generated_structures'\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # print(f\"Using device: {device}\") # Suppressed\n",
        "\n",
        "    # --- Model Dims (MUST match your training setup) ---\n",
        "    HIDDEN_DIM = 256\n",
        "    LATENT_DIM = 1024\n",
        "    NUM_LAYERS = 3\n",
        "\n",
        "    # --- ESM-2 Configuration ---\n",
        "    ESM_MODEL_NAME = 'esm2_t12_35M_UR50D' # 35 Million Parameter\n",
        "    #ESM_MODEL_NAME = 'esm2_t30_150M_UR50D' #150 Million Parameter\n",
        "    #ESM_MODEL_NAME = 'esm2_t33_650M_UR50D' # 650 Million Parameter\n",
        "    MAX_PROTEIN_SEQUENCE_LEN = 1000\n",
        "\n",
        "    # --- ChemBERTa Configuration ---\n",
        "    CHEMBERTA_MODEL_NAME = 'DeepChem/ChemBERTa-77M-MTR'\n",
        "\n",
        "    # ----------------------\n",
        "    # Load Tokenizers and Base Models\n",
        "    # ----------------------\n",
        "    try:\n",
        "        # print(f\"Loading ESM-2 model: {ESM_MODEL_NAME}...\") # Suppressed\n",
        "        esm_tokenizer = AutoTokenizer.from_pretrained(f\"facebook/{ESM_MODEL_NAME}\")\n",
        "        esm_model = EsmModel.from_pretrained(f\"facebook/{ESM_MODEL_NAME}\").to(device).eval()\n",
        "        ESM_EMBEDDING_DIM = esm_model.config.hidden_size\n",
        "        ESM_AVAILABLE = True\n",
        "    except Exception as e:\n",
        "        ESM_AVAILABLE = False\n",
        "        ESM_EMBEDDING_DIM = None\n",
        "        # print(f\"WARNING: ESM-2 not available ({e}). New sequences cannot be processed.\") # Suppressed\n",
        "\n",
        "    # print(f\"Loading ChemBERTa tokenizer and model: {CHEMBERTA_MODEL_NAME}...\") # Suppressed\n",
        "    smiles_tokenizer = AutoTokenizer.from_pretrained(CHEMBERTA_MODEL_NAME)\n",
        "    smiles_model = AutoModel.from_pretrained(CHEMBERTA_MODEL_NAME)\n",
        "    SMILES_EMBEDDING_DIM = smiles_model.config.hidden_size\n",
        "    SMILES_VOCAB_SIZE = smiles_tokenizer.vocab_size\n",
        "\n",
        "    PAD_ID = smiles_tokenizer.pad_token_id\n",
        "    BOS_ID = smiles_tokenizer.bos_token_id or smiles_tokenizer.cls_token_id\n",
        "    EOS_ID = smiles_tokenizer.eos_token_id or smiles_tokenizer.sep_token_id\n",
        "\n",
        "    # Infer the protein embedding dimension\n",
        "    if ESM_EMBEDDING_DIM is None:\n",
        "        try:\n",
        "            all_protein_embeddings = np.load(EMBEDDING_SAVE_PATH, allow_pickle=True)\n",
        "            protein_emb_dim = int(np.array(all_protein_embeddings[0]).shape[-1])\n",
        "            # print(f\"Inferred protein embedding dim from .npy: {protein_emb_dim}\") # Suppressed\n",
        "        except Exception:\n",
        "            raise RuntimeError(\"Could not load ESM or infer protein dimension from .npy.\")\n",
        "    else:\n",
        "        protein_emb_dim = ESM_EMBEDDING_DIM\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. UTILITY AND MODEL CLASSES (No print statements in definitions)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def safe_to_numpy(x):\n",
        "    if isinstance(x, np.ndarray): return x\n",
        "    return np.array(x)\n",
        "\n",
        "def load_seq_index():\n",
        "    if not os.path.exists(SEQ_INDEX_PATH):\n",
        "        return {}\n",
        "    with open(SEQ_INDEX_PATH, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_esm_embedding_pooled(seq: str) -> np.ndarray:\n",
        "    \"\"\"Computes pooled ESM embedding for a new sequence (Novel Sequence Fallback).\"\"\"\n",
        "    if not ESM_AVAILABLE:\n",
        "        raise RuntimeError(\"ESM not available to compute embeddings.\")\n",
        "    seq_spaced = \" \".join(seq)\n",
        "    encoded = esm_tokenizer(seq_spaced, return_tensors='pt',\n",
        "                             max_length=MAX_PROTEIN_SEQUENCE_LEN,\n",
        "                             padding=True, truncation=True)\n",
        "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "    out = esm_model(**encoded)\n",
        "    emb = out.last_hidden_state.mean(dim=1).squeeze().detach().cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "def get_protein_embedding_for_seq(seq, seq_to_idx, all_protein_embeddings):\n",
        "    \"\"\"Retrieves embedding (DB Look-up for training data or ESM Fallback for novel sequences).\"\"\"\n",
        "    if seq in seq_to_idx:\n",
        "        print(\"Status: Sequence found in training index. Using precomputed embedding.\")\n",
        "        emb = safe_to_numpy(all_protein_embeddings[seq_to_idx[seq]])\n",
        "        return torch.tensor(emb, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    if ESM_AVAILABLE:\n",
        "        print(\"Status: Sequence is new. Computing ESM-2 embedding on-the-fly...\")\n",
        "        emb = get_esm_embedding_pooled(seq)\n",
        "        return torch.tensor(emb, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    raise RuntimeError(\n",
        "        \"Cannot embed unseen protein sequence: No precomputed embedding and ESM-2 is unavailable. \"\n",
        "    )\n",
        "\n",
        "def draw_smiles_to_file(smi_list, output_dir=OUTPUT_DIR):\n",
        "    \"\"\"Draws molecular structures for a list of valid SMILES, saves them, and displays them (using IPython display).\"\"\"\n",
        "    if not smi_list:\n",
        "        return\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    mols = [Chem.MolFromSmiles(smi) for smi in smi_list]\n",
        "    mols = [mol for mol in mols if mol is not None]\n",
        "\n",
        "    if not mols:\n",
        "        print(\"Could not convert any valid SMILES to RDKit molecules for drawing.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nDrawing, saving, and displaying {len(mols)} valid molecular structures...\")\n",
        "\n",
        "    # Draw and save/show each structure individually with an index\n",
        "    for i, mol in enumerate(mols):\n",
        "        try:\n",
        "            img = Draw.MolToImage(mol, size=(300, 300))\n",
        "\n",
        "            filename = os.path.join(output_dir, f'molecule_{i + 1}.png')\n",
        "            img.save(filename) # Saves the file\n",
        "\n",
        "            # --- COLAB/NOTEBOOK DISPLAY: Use IPython.display ---\n",
        "            print(f\"Displaying structure {i+1}...\")\n",
        "            display(img)\n",
        "            # ----------------------------------------------------\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing molecule {i+1}: {e}\")\n",
        "            print(\"Note: Image display failed. Check environment configuration.\")\n",
        "\n",
        "# --- Model Architecture ---\n",
        "class FiLM(nn.Module):\n",
        "    def __init__(self, cond_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Linear(cond_dim, hidden_dim)\n",
        "        self.beta  = nn.Linear(cond_dim, hidden_dim)\n",
        "    def forward(self, x, cond):\n",
        "        g = self.gamma(cond).unsqueeze(1)\n",
        "        b = self.beta(cond).unsqueeze(1)\n",
        "        return x * (1 + g) + b\n",
        "\n",
        "class ConditionalVAE(nn.Module):\n",
        "    def __init__(self, smiles_embedding_layer, vocab_size, smiles_embedding_dim,\n",
        "                 protein_embedding_dim, hidden_dim, latent_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Architecture setup (matching your training code)\n",
        "        self.smiles_embedding_layer = smiles_embedding_layer\n",
        "        self.encoder_rnn = nn.GRU(smiles_embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.encoder_protein_mlp = nn.Sequential(nn.Linear(protein_embedding_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.decoder_init = nn.Sequential(nn.Linear(latent_dim + protein_embedding_dim, hidden_dim), nn.ReLU())\n",
        "        self.decoder_rnn = nn.GRU(smiles_embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.film = FiLM(protein_embedding_dim, hidden_dim)\n",
        "        self.out_proj = nn.Linear(hidden_dim, smiles_embedding_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(smiles_embedding_dim, vocab_size, bias=False)\n",
        "        self.fc_out.weight = self.smiles_embedding_layer.weight\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, protein_embedding, max_len, top_k, top_p, temperature):\n",
        "        B = protein_embedding.size(0)\n",
        "        z = torch.randn(B, self.latent_dim, device=protein_embedding.device)\n",
        "        zc = torch.cat([z, protein_embedding], dim=1)\n",
        "        h = self.decoder_init(zc).unsqueeze(0).repeat(self.decoder_rnn.num_layers, 1, 1)\n",
        "\n",
        "        cur = torch.full((B, 1), BOS_ID, dtype=torch.long, device=protein_embedding.device)\n",
        "        seq = [cur]\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            emb = self.smiles_embedding_layer(cur[:, -1:])\n",
        "            out, h = self.decoder_rnn(emb, h)\n",
        "            out = self.film(out, protein_embedding)\n",
        "            out = self.out_proj(out)\n",
        "            logits = self.fc_out(out[:, -1, :]) / max(1e-8, temperature)\n",
        "\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            next_tok = self._sample_top_k_top_p(probs, top_k, top_p).unsqueeze(1)\n",
        "            seq.append(next_tok)\n",
        "            cur = torch.cat([cur, next_tok], dim=1)\n",
        "            if torch.all(next_tok.squeeze(1) == EOS_ID):\n",
        "                break\n",
        "        return torch.cat(seq, dim=1)\n",
        "\n",
        "    def _sample_top_k_top_p(self, probs, top_k, top_p):\n",
        "        B, V = probs.size()\n",
        "        if top_k is not None and top_k > 0:\n",
        "            topk_vals, topk_idx = torch.topk(probs, k=min(top_k, V), dim=-1)\n",
        "            mask = torch.zeros_like(probs).scatter_(1, topk_idx, 1.0)\n",
        "            probs = probs * mask\n",
        "        if top_p is not None and 0.0 < top_p < 1.0:\n",
        "            sorted_probs, sorted_idx = torch.sort(probs, descending=True, dim=-1)\n",
        "            cum = torch.cumsum(sorted_probs, dim=-1)\n",
        "            cutoff = cum > top_p\n",
        "            cutoff[..., 0] = False\n",
        "            sorted_probs[cutoff] = 0.0\n",
        "            probs = torch.zeros_like(probs).scatter(1, sorted_idx, sorted_probs)\n",
        "\n",
        "        probs = probs / probs.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "        return torch.multinomial(probs, num_samples=1).squeeze(1)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. MAIN PREDICTION FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_smiles_for_sequence(raw_sequence: str, num_samples: int, max_len: int, temperature: float, top_k: int = 50, top_p: float = 0.99) -> list[str]:\n",
        "    \"\"\"\n",
        "    Generates and validates SMILES molecules for a given protein sequence.\n",
        "    Returns a list of valid SMILES strings.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(BEST_MODEL_PATH):\n",
        "        raise FileNotFoundError(f\"Best model not found at {BEST_MODEL_PATH}. Train first!\")\n",
        "\n",
        "    # 1. Load data artifacts\n",
        "    seq_to_idx = load_seq_index()\n",
        "    try:\n",
        "        all_protein_embeddings = np.load(EMBEDDING_SAVE_PATH, allow_pickle=True)\n",
        "    except FileNotFoundError:\n",
        "        if not ESM_AVAILABLE:\n",
        "              raise FileNotFoundError(f\"Embedding file {EMBEDDING_SAVE_PATH} not found and ESM unavailable.\")\n",
        "        all_protein_embeddings = []\n",
        "\n",
        "    # 2. Rebuild and Load Model\n",
        "    pre_emb = smiles_model.get_input_embeddings().weight.detach().clone()\n",
        "    smiles_embedding_layer = nn.Embedding.from_pretrained(pre_emb, freeze=False)\n",
        "\n",
        "    model = ConditionalVAE(\n",
        "        smiles_embedding_layer=smiles_embedding_layer,\n",
        "        vocab_size=SMILES_VOCAB_SIZE,\n",
        "        smiles_embedding_dim=SMILES_EMBEDDING_DIM,\n",
        "        protein_embedding_dim=protein_emb_dim,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        latent_dim=LATENT_DIM,\n",
        "        num_layers=NUM_LAYERS\n",
        "    ).to(device)\n",
        "    model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 3. Get embedding for the input sequence (Handles novel sequences via ESM-2)\n",
        "    try:\n",
        "        pe = get_protein_embedding_for_seq(raw_sequence, seq_to_idx, all_protein_embeddings)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"ERROR: {e}\")\n",
        "        return []\n",
        "\n",
        "    # 4. Generate with controlled parameters\n",
        "    valid_smiles_list = []\n",
        "\n",
        "    print(f\"  Samples: {num_samples} | Max Len: {max_len} tokens | Temp: {temperature} | Top-P: {top_p}\")\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        ids = model.generate(pe, max_len=max_len, top_k=top_k, top_p=top_p, temperature=temperature)\n",
        "        smi = smiles_tokenizer.decode(ids[0].tolist(), skip_special_tokens=True)\n",
        "\n",
        "        # Validation Check\n",
        "        is_valid = Chem.MolFromSmiles(smi) is not None\n",
        "        validity = \"VALID\" if is_valid else \"INVALID\"\n",
        "\n",
        "        if is_valid:\n",
        "            valid_smiles_list.append(smi)\n",
        "\n",
        "        print(f\"  Sample {i+1} of {num_samples}: [{validity}] {smi}\")\n",
        "\n",
        "    return valid_smiles_list\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. MAIN EXECUTION BLOCK (User Interface)\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- Recommended Default Controls ---\n",
        "    DEFAULT_MAX_LEN = 90\n",
        "    DEFAULT_MIN_LEN = 30 # New default for range mode\n",
        "    DEFAULT_TEMP = 0.8\n",
        "    DEFAULT_SAMPLES = 10\n",
        "\n",
        "    # This is the first print statement visible to the user\n",
        "    print(\"\\n--- Conditional VAE Molecule Generation Interface ---\")\n",
        "\n",
        "    # In a Colab notebook, you will need to upload your model files first.\n",
        "    raw_sequence = input(\"Enter the new protein sequence: \\n> \")\n",
        "\n",
        "    try:\n",
        "        num_samples = int(input(f\"Enter number of SMILES to generate per length (default: {DEFAULT_SAMPLES}): \") or DEFAULT_SAMPLES)\n",
        "        temperature = float(input(f\"Enter Temperature (e.g., 0.8 for high validity, 1.2 for high diversity, default: {DEFAULT_TEMP}): \") or DEFAULT_TEMP)\n",
        "    except ValueError:\n",
        "        print(\"Invalid number or temperature entered. Using defaults for samples/temp.\")\n",
        "        num_samples = DEFAULT_SAMPLES\n",
        "        temperature = DEFAULT_TEMP\n",
        "\n",
        "    # --- NEW LENGTH LOGIC ---\n",
        "    print(\"\\n--- SMILES LENGTH MODE ---\")\n",
        "    length_mode = input(\"Select generation mode: [F]ixed Length or [V]ariable Length Range? (Default: F): \")\n",
        "    length_mode = length_mode.strip().upper()\n",
        "\n",
        "    generation_lengths = []\n",
        "\n",
        "    if length_mode == 'V':\n",
        "        try:\n",
        "            min_len = int(input(f\"Enter Minimum SMILES Length in tokens (default: {DEFAULT_MIN_LEN}): \") or DEFAULT_MIN_LEN)\n",
        "            max_len = int(input(f\"Enter Maximum SMILES Length in tokens (default: {DEFAULT_MAX_LEN}): \") or DEFAULT_MAX_LEN)\n",
        "\n",
        "            if min_len > max_len:\n",
        "                print(\"Warning: Minimum length cannot be greater than maximum. Using fixed default length.\")\n",
        "                generation_lengths = [DEFAULT_MAX_LEN]\n",
        "            else:\n",
        "                # Generate a list of lengths from min_len to max_len, stepping by 5\n",
        "                generation_lengths = list(range(min_len, max_len + 1, 5))\n",
        "                print(f\"Set to generate for lengths: {min_len} to {max_len} (stepping by 5).\")\n",
        "                print(f\"Generation lengths: {generation_lengths}\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid length entered. Using fixed default length.\")\n",
        "            generation_lengths = [DEFAULT_MAX_LEN]\n",
        "\n",
        "    else: # Default to Fixed Length (or invalid input)\n",
        "        try:\n",
        "            max_len = int(input(f\"Enter Max SMILES Length in tokens (default: {DEFAULT_MAX_LEN}): \") or DEFAULT_MAX_LEN)\n",
        "            generation_lengths = [max_len]\n",
        "            print(f\"Set to generate for fixed length: {max_len}.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid length entered. Using fixed default length.\")\n",
        "            generation_lengths = [DEFAULT_MAX_LEN]\n",
        "\n",
        "    # --- Execution Loop ---\n",
        "    print(\"\\nStarting generation process...\")\n",
        "\n",
        "    all_valid_smiles = []\n",
        "    total_generated = 0\n",
        "\n",
        "    for current_max_len in generation_lengths:\n",
        "        # NOTE: The generate function uses max_len as the termination condition.\n",
        "        # We call it sequentially for each length in the desired range.\n",
        "        print(f\"\\n=======================================================\")\n",
        "        print(f\"| RUNNING: Max Length = {current_max_len} tokens |\")\n",
        "        print(f\"=======================================================\")\n",
        "\n",
        "        # Run the generation for the current max_len\n",
        "        valid_smiles = generate_smiles_for_sequence(\n",
        "            raw_sequence.strip(),\n",
        "            num_samples,\n",
        "            current_max_len,\n",
        "            temperature\n",
        "        )\n",
        "\n",
        "        all_valid_smiles.extend(valid_smiles)\n",
        "        total_generated += num_samples\n",
        "\n",
        "    # --- Final Output and Drawing ---\n",
        "    total_valid = len(all_valid_smiles)\n",
        "\n",
        "    if all_valid_smiles:\n",
        "        print(f\"\\n==========================================\")\n",
        "        print(f\"--- FINAL RESULTS (Combined from {len(generation_lengths)} length range(s)) ---\")\n",
        "        print(f\"Total Samples Generated: {total_generated}\")\n",
        "        print(f\"Total Valid Samples: {total_valid}\")\n",
        "        print(f\"Validity Percentage (across all): {total_valid / total_generated * 100:.2f}%\")\n",
        "        print(\"==========================================\")\n",
        "\n",
        "        # 1) List of Final, Valid SMILES\n",
        "        print(\"\\nFinal List of Valid SMILES:\")\n",
        "        for i, smi in enumerate(all_valid_smiles):\n",
        "            print(f\"  {i+1}. {smi}\")\n",
        "\n",
        "        # 2) Draw, SAVE, and SHOW Structures\n",
        "        draw_smiles_to_file(all_valid_smiles)\n",
        "        print(f\"\\nâœ… Structures saved to the '{OUTPUT_DIR}' directory and displayed!\")\n",
        "    else:\n",
        "        print(\"\\nGeneration yielded no valid SMILES.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Comprehensive Pre-Docking Analysis\n",
        "\n",
        "# --- INSTALL/IMPORT ALL NECESSARY LIBRARIES ---\n",
        "# Suppressing installation output\n",
        "!pip install pandas > /dev/null 2>&1\n",
        "!pip install rdkit > /dev/null 2>&1\n",
        "!pip install matplotlib seaborn > /dev/null 2>&1\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from random import uniform\n",
        "from IPython.display import display, Markdown, Image\n",
        "from PIL import Image as PILImage\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from rdkit import Chem\n",
        "# QED is accessed via Descriptors.qed(mol)\n",
        "from rdkit.Chem import Descriptors, AllChem, Draw\n",
        "from rdkit.Chem.FilterCatalog import FilterCatalogParams, FilterCatalog\n",
        "from rdkit.DataStructs import BulkTanimotoSimilarity, TanimotoSimilarity\n",
        "from rdkit.Chem.rdFMCS import FindMCS\n",
        "from rdkit.Chem.rdMolDescriptors import CalcNumSpiroAtoms, CalcNumBridgeheadAtoms, CalcNumAmideBonds # Used for SA Score\n",
        "\n",
        "# --- Suppress RDKit specific warnings ---\n",
        "from rdkit import RDLogger\n",
        "rdLogger = RDLogger.logger()\n",
        "rdLogger.setLevel(RDLogger.CRITICAL)\n",
        "\n",
        "# ==============================================================================\n",
        "# --- SA SCORE HELPER FUNCTION (Ensures self-contained script) ---\n",
        "# NOTE: This implementation is adapted from the RDKit contributor script (sascorer.py)\n",
        "# to make the notebook self-contained.\n",
        "# ==============================================================================\n",
        "def calculate_sa_score(mol):\n",
        "    \"\"\"Calculates the Synthetic Accessibility Score (SA Score) for a molecule.\"\"\"\n",
        "    # These imports are needed specifically for the SA Score calculation logic\n",
        "    from rdkit.Chem.Scaffolds import MurckoScaffold\n",
        "    from rdkit.Chem.rdMolDescriptors import CalcNumSpiroAtoms, CalcNumBridgeheadAtoms\n",
        "\n",
        "    # RDKit's SA Score function is complex. We'll use a functional proxy that leverages\n",
        "    # fragment counts and complexity features, which is the spirit of SA score.\n",
        "\n",
        "    # Component 1: Fragment Score (Usually requires the full fragment library, skipped here for simplicity)\n",
        "    # Component 2: Complexity Score (Number of rings, heteroatoms, spiro/bridgehead atoms, etc.)\n",
        "\n",
        "    complexity = 0\n",
        "    complexity += mol.GetNumAtoms() / 100.0 # Atoms count penalty\n",
        "    complexity += Descriptors.HeavyAtomCount(mol) / 100.0\n",
        "    complexity += Descriptors.RingCount(mol) * 0.5\n",
        "    complexity += CalcNumSpiroAtoms(mol) * 1.0\n",
        "    complexity += CalcNumBridgeheadAtoms(mol) * 1.5\n",
        "\n",
        "    # Component 3: Penalties (e.g., number of stereocenters, large rings)\n",
        "    chiral_centers = len(Chem.FindMolChiralCenters(mol, includeUnassigned=True))\n",
        "    complexity += chiral_centers * 1.0\n",
        "\n",
        "    # SA Score is often presented on a scale of 1 (easy) to 10 (hard)\n",
        "    # This formula is a heuristic proxy (lower is better, easier to synthesize)\n",
        "    sa_score = 10 - (10 * np.exp(-complexity / 5.0)) # Maps complexity to the 1-10 range\n",
        "\n",
        "    # Ensure score is within bounds [1, 10]\n",
        "    return max(1.0, min(10.0, sa_score))\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "# 0. INITIAL DATA CHECK AND SETUP (FORCES USE OF GENERATED SMILES)\n",
        "smiles_list_raw = globals().get('all_valid_smiles') or globals().get('valid_smiles')\n",
        "\n",
        "if not smiles_list_raw:\n",
        "    print(\"FATAL ERROR: The required generated SMILES list (`all_valid_smiles` or `valid_smiles`) is NOT available in the global environment.\")\n",
        "    print(\"Please ensure the CVAE generation cell has run successfully and populated the global variable.\")\n",
        "    sys.exit()\n",
        "\n",
        "print(f\"âœ… Starting pipeline analysis on a combined total of {len(smiles_list_raw)} valid SMILES molecules (from CVAE output).\")\n",
        "current_smiles = smiles_list_raw\n",
        "valid_mols = [Chem.MolFromSmiles(s) for s in current_smiles]\n",
        "valid_mols_final = [m for m in valid_mols if m is not None]\n",
        "current_smiles_final = [s for s, m in zip(current_smiles, valid_mols) if m is not None]\n",
        "\n",
        "if not valid_mols_final:\n",
        "    print(\"FATAL ERROR: Although SMILES were found, no valid RDKit molecules could be generated.\")\n",
        "    sys.exit()\n",
        "\n",
        "# Set up the DataFrame base for consistency\n",
        "df_base = pd.DataFrame({'SMILES': current_smiles_final})\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. RO5, QED, and PAINS ANALYSIS (Updates df_filtered)\n",
        "print(\"\\n--- STAGE 1: Drug-Likeness (Ro5, QED & PAINS) Analysis ---\")\n",
        "\n",
        "LIPINSKI_CRITERIA = {'MW': 500, 'LogP': 5.0, 'HBD': 5, 'HBA': 10}\n",
        "params = FilterCatalogParams()\n",
        "params.AddCatalog(FilterCatalogParams.FilterCatalogs.PAINS)\n",
        "pains_catalog = FilterCatalog(params)\n",
        "\n",
        "def check_lipinski_rule(mol):\n",
        "    # Calculate TPSA, Fsp3, Rotatable bonds for additional analysis\n",
        "    mw, logp, hbd, hba = Descriptors.MolWt(mol), Descriptors.MolLogP(mol), Descriptors.NumHDonors(mol), Descriptors.NumHAcceptors(mol)\n",
        "    tpsa = Descriptors.TPSA(mol)\n",
        "    fsp3 = Descriptors.FractionCSP3(mol)\n",
        "    rot_bonds = Descriptors.NumRotatableBonds(mol)\n",
        "    qed_score = Descriptors.qed(mol) # --- NEW: QED Calculation ---\n",
        "\n",
        "    violations = sum([\n",
        "        mw > LIPINSKI_CRITERIA['MW'], logp > LIPINSKI_CRITERIA['LogP'],\n",
        "        hbd > LIPINSKI_CRITERIA['HBD'], hba > LIPINSKI_CRITERIA['HBA']\n",
        "    ])\n",
        "    ro5_pass = violations <= 1\n",
        "    # Return QED score as well\n",
        "    return mw, logp, hbd, hba, tpsa, fsp3, rot_bonds, violations, ro5_pass, qed_score\n",
        "\n",
        "def check_pains(mol):\n",
        "    matches = pains_catalog.GetMatches(mol)\n",
        "    is_pains = len(matches) > 0\n",
        "    pains_descriptions = [match.GetDescription() for match in matches]\n",
        "    return is_pains, \"; \".join(pains_descriptions)\n",
        "\n",
        "ro5_results = []\n",
        "for smi in df_base['SMILES']:\n",
        "    mol = Chem.MolFromSmiles(smi)\n",
        "    if mol is None: continue\n",
        "\n",
        "    # Unpack QED score\n",
        "    mw, logp, hbd, hba, tpsa, fsp3, rot_bonds, ro5_viol, ro5_pass, qed_score = check_lipinski_rule(mol)\n",
        "    is_pains, pains_desc = check_pains(mol)\n",
        "\n",
        "    ro5_results.append({\n",
        "        'SMILES': smi,\n",
        "        'MW': f\"{mw:.2f}\",\n",
        "        'LogP': f\"{logp:.2f}\",\n",
        "        'HBD': hbd,\n",
        "        'HBA': hba,\n",
        "        'TPSA': f\"{tpsa:.2f}\",\n",
        "        'Fsp3': f\"{fsp3:.3f}\",\n",
        "        'Rot. Bonds': rot_bonds,\n",
        "        'Ro5 Violations': ro5_viol,\n",
        "        'Ro5 Pass': ro5_pass,\n",
        "        'Is PAINS': is_pains,\n",
        "        'PAINS Desc': pains_desc,\n",
        "        'QED_Value': qed_score, # For sorting\n",
        "        'QED Score': f\"{qed_score:.3f}\", # For display\n",
        "        'Final Pass': ro5_pass and not is_pains\n",
        "    })\n",
        "\n",
        "df_filtered = pd.DataFrame(ro5_results)\n",
        "globals()['df_filtered'] = df_filtered\n",
        "\n",
        "print(f\"   -> Ro5 Pass: {df_filtered['Ro5 Pass'].sum()} | No PAINS Alert: {len(df_filtered) - df_filtered['Is PAINS'].sum()}\")\n",
        "print(f\"   -> Average QED Score: {df_filtered['QED_Value'].mean():.3f}\")\n",
        "print(\"   -> Drug-Likeness Analysis Complete.\")\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "# 2. DIVERSITY, NOVELTY, TOXICITY, AND SA SCORE ANALYSIS (Updates df_output and avg_internal_distance)\n",
        "print(\"\\n--- STAGE 2: Diversity, Novelty, Toxicity, and SA Score Analysis ---\")\n",
        "\n",
        "file_name = '/content/unique_smiles_list.csv' # <-- **CHANGE THIS TO YOUR TRAINING DATA CSV**\n",
        "smiles_column_name = 'SMILES'\n",
        "sample_size = 100000\n",
        "\n",
        "training_smiles = []\n",
        "try:\n",
        "    df_training = pd.read_csv(file_name, usecols=[smiles_column_name]).dropna(subset=[smiles_column_name])\n",
        "    effective_sample_size = min(sample_size, len(df_training))\n",
        "    training_smiles = df_training.sample(n=effective_sample_size, random_state=42)[smiles_column_name].tolist()\n",
        "    print(f\"   -> Loaded {len(training_smiles)} training SMILES for Novelty calculation.\")\n",
        "except Exception:\n",
        "    training_smiles = ['CC(=O)Oc1ccccc1C(=O)O'] * 100\n",
        "    print(f\"   -> WARNING: Training data file '{file_name}' not found. Using minimal mock SMILES for Novelty.\")\n",
        "\n",
        "def calculate_novelty(query_fps, training_smiles):\n",
        "    if not training_smiles: return [0.0] * len(query_fps)\n",
        "    training_mols = [Chem.MolFromSmiles(s) for s in training_smiles if Chem.MolFromSmiles(s) is not None]\n",
        "    training_fps = [AllChem.GetMorganFingerprint(m, 2) for m in training_mols]\n",
        "    max_sims = []\n",
        "    for fp in query_fps:\n",
        "        sims = BulkTanimotoSimilarity(fp, training_fps)\n",
        "        max_sims.append(max(sims))\n",
        "    return max_sims\n",
        "\n",
        "def calculate_diversity(query_fps):\n",
        "    n = len(query_fps)\n",
        "    if n < 2: return 0.0\n",
        "    total_distance = sum((1.0 - TanimotoSimilarity(query_fps[i], query_fps[j])) for i in range(n) for j in range(i + 1, n))\n",
        "    count = n * (n - 1) / 2\n",
        "    return total_distance / count\n",
        "\n",
        "def check_tox_alerts(mol):\n",
        "    alerts = {'[#6]C#N': 'Cyano Group', '[#7](=O)=O': 'Nitro Group'}\n",
        "    is_alert, alert_descriptions = False, []\n",
        "    for smarts, desc in alerts.items():\n",
        "        if mol.HasSubstructMatch(Chem.MolFromSmarts(smarts)):\n",
        "            is_alert = True\n",
        "            alert_descriptions.append(desc)\n",
        "    return is_alert, \"; \".join(alert_descriptions)\n",
        "\n",
        "fps = [AllChem.GetMorganFingerprint(m, 2) for m in valid_mols_final]\n",
        "max_novelty_sims = calculate_novelty(fps, training_smiles)\n",
        "avg_internal_distance = calculate_diversity(fps)\n",
        "globals()['avg_internal_distance'] = avg_internal_distance\n",
        "\n",
        "analysis_results = []\n",
        "for i, mol in enumerate(valid_mols_final):\n",
        "    is_tox_alert, tox_desc = check_tox_alerts(mol)\n",
        "    novelty_sim = max_novelty_sims[i] if training_smiles else 1.0\n",
        "    novelty_score_display = f\"{novelty_sim:.3f}\" if training_smiles else \"0.000 (Skipped)\"\n",
        "\n",
        "    # --- SA Score Calculation ---\n",
        "    sa_score = calculate_sa_score(mol)\n",
        "    # -------------------------------\n",
        "\n",
        "    initial_priority = 'MEDIUM'\n",
        "    if is_tox_alert:\n",
        "        initial_priority = 'REJECTED (Tox Alert)'\n",
        "\n",
        "    analysis_results.append({\n",
        "        'SMILES': current_smiles_final[i],\n",
        "        'Max Novelty Sim': novelty_score_display,\n",
        "        'Tox Alert': is_tox_alert,\n",
        "        'Tox Alert Desc': tox_desc,\n",
        "        'FINAL RDKIT PRIORITY': initial_priority,\n",
        "        'Sort_Novelty_Value': novelty_sim,\n",
        "        'SA Score': sa_score # --- NEW COLUMN ---\n",
        "    })\n",
        "\n",
        "df_output_novelty = pd.DataFrame(analysis_results)\n",
        "\n",
        "# Merge with the full property table (df_filtered)\n",
        "df_output = df_output_novelty.merge(\n",
        "    df_filtered[['SMILES', 'MW', 'LogP', 'TPSA', 'Fsp3', 'Rot. Bonds', 'QED_Value']], # ADD QED_Value\n",
        "    on='SMILES',\n",
        "    how='left'\n",
        ")\n",
        "globals()['df_output'] = df_output\n",
        "\n",
        "print(f\"   -> Average Tanimoto Distance (Diversity): {avg_internal_distance:.3f}\")\n",
        "print(\"   -> Novelty, Toxicity, and SA Score Analysis Complete.\")\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "# 3. MAXIMAL COMMON SUBSTRUCTURE (MCS) ANALYSIS\n",
        "print(\"\\n--- STAGE 3: Maximal Common Substructure (MCS) Analysis ---\")\n",
        "\n",
        "def perform_mcs_analysis(mols: list):\n",
        "    if len(mols) < 2:\n",
        "        print(f\"   -> MCS skipped: Found only {len(mols)} valid molecule(s).\")\n",
        "        return None, None\n",
        "\n",
        "    mcs_result = FindMCS(mols, timeout=60, atomCompare=Chem.rdFMCS.AtomCompare.CompareElements)\n",
        "    mcs_smarts = mcs_result.smartsString\n",
        "    mcs_mol = Chem.MolFromSmarts(mcs_smarts)\n",
        "\n",
        "    print(f\"   -> MCS SMARTS: {mcs_smarts}\")\n",
        "\n",
        "    return mcs_mol, mcs_smarts\n",
        "\n",
        "mcs_mol, mcs_smarts = perform_mcs_analysis(valid_mols_final)\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "# 4. RDKIT-BASED PRIORITIZATION (Final Priority Assignment)\n",
        "print(\"\\n--- STAGE 4: Final RDKit-Based Prioritization ---\")\n",
        "\n",
        "df_combined = df_output.merge(\n",
        "    df_filtered[['SMILES', 'Ro5 Pass', 'Is PAINS', 'PAINS Desc', 'QED Score']], # Add QED Score for display\n",
        "    on='SMILES',\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# Handle missing values and convert to numeric\n",
        "df_combined['TPSA'] = pd.to_numeric(df_combined['TPSA'], errors='coerce').fillna(999.0)\n",
        "df_combined['MW'] = pd.to_numeric(df_combined['MW'], errors='coerce').fillna(999.0)\n",
        "df_combined['LogP'] = pd.to_numeric(df_combined['LogP'], errors='coerce').fillna(999.0)\n",
        "df_combined['Rot. Bonds'] = pd.to_numeric(df_combined['Rot. Bonds'], errors='coerce').fillna(999.0)\n",
        "df_combined['Max Novelty Sim'] = pd.to_numeric(df_combined['Max Novelty Sim'], errors='coerce').fillna(1.0)\n",
        "df_combined['SA Score'] = pd.to_numeric(df_combined['SA Score'], errors='coerce').fillna(10.0) # SA Score Max is 10 (hard)\n",
        "df_combined['QED_Value'] = pd.to_numeric(df_combined['QED_Value'], errors='coerce').fillna(0.0) # --- NEW: QED Value ---\n",
        "\n",
        "# Rejection Criteria\n",
        "df_combined.loc[\n",
        "    (df_combined['Ro5 Pass'] == False) | (df_combined['Is PAINS'] == True),\n",
        "    'FINAL RDKIT PRIORITY'\n",
        "] = 'REJECTED (Ro5/PAINS)'\n",
        "df_combined.loc[\n",
        "    df_combined['Tox Alert'] == True,\n",
        "    'FINAL RDKIT PRIORITY'\n",
        "] = 'REJECTED (Tox Alert)'\n",
        "\n",
        "df_compliant = df_combined[\n",
        "    ~df_combined['FINAL RDKIT PRIORITY'].str.startswith('REJECTED')\n",
        "].copy()\n",
        "\n",
        "# --- UPDATED SORTING CRITERIA: HIGHER QED, Lower TPSA, Lower SA Score, Lower Novelty Sim is better ---\n",
        "df_compliant = df_compliant.sort_values(\n",
        "    by=['QED_Value', 'TPSA', 'SA Score', 'Sort_Novelty_Value'],\n",
        "    ascending=[False, True, True, True] # False for QED (Higher is Better), True for others (Lower is Better)\n",
        ")\n",
        "\n",
        "top_n_compliant = max(1, min(10, len(df_compliant) // 10))\n",
        "\n",
        "# Assign final priorities\n",
        "df_combined.loc[df_compliant.index, 'FINAL RDKIT PRIORITY'] = 'LOW'\n",
        "df_combined.loc[df_compliant.index[:top_n_compliant], 'FINAL RDKIT PRIORITY'] = 'HIGH'\n",
        "df_combined.loc[df_compliant.index[top_n_compliant:top_n_compliant*2], 'FINAL RDKIT PRIORITY'] = 'MEDIUM'\n",
        "\n",
        "# Final sorting for display\n",
        "priority_order = pd.CategoricalDtype(['HIGH', 'MEDIUM', 'LOW', 'REJECTED (Tox Alert)', 'REJECTED (Ro5/PAINS)'], ordered=True)\n",
        "df_combined['FINAL RDKIT PRIORITY'] = df_combined['FINAL RDKIT PRIORITY'].astype(priority_order)\n",
        "\n",
        "df_combined = df_combined.sort_values(\n",
        "    by=['FINAL RDKIT PRIORITY', 'QED_Value', 'SA Score', 'TPSA', 'Sort_Novelty_Value'], # QED_Value added here\n",
        "    ascending=[True, False, True, True, True] # Priority (HIGH first), then QED (Higher is Better), then SA (Easy first), then TPSA, then Novelty\n",
        ")\n",
        "globals()['df_combined'] = df_combined # Ensure df_combined is available globally\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "# 5. PRINT FINAL REPORT\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"             ðŸ† FINAL DRUG DISCOVERY REPORT - SUMMARY MATRIX ðŸ†\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "summary_data = {\n",
        "    'Metric Category': ['Generation', 'Drug-Likeness', 'Drug-Likeness', 'Physicochemical', 'Synthesizability', 'Synthesizability', 'Chemical Space', 'Toxicity/Priority', 'Final Leads'],\n",
        "    'Metric Name': [\n",
        "        'Total Unique Valid Molecules',\n",
        "        'Average QED Score (Higher is Better)', # --- NEW ---\n",
        "        'Best QED Score (Max)', # --- NEW ---\n",
        "        'Average TPSA ($\\AA^2$)',\n",
        "        'Average SA Score (1=Easy, 10=Hard)',\n",
        "        'Best SA Score (Easiest)',\n",
        "        'Maximal Diversity (Tanimoto Distance)',\n",
        "        'Molecules Marked HIGH RDKIT Priority',\n",
        "        'Final High-Confidence Leads (Ro5 Pass & Top RDKit Priority)'\n",
        "    ],\n",
        "    'Value': [\n",
        "        f'{len(smiles_list_raw)}',\n",
        "        f'{df_combined[\"QED_Value\"].mean():.3f}', # --- NEW ---\n",
        "        f'{df_combined[\"QED_Value\"].max():.3f}', # --- NEW ---\n",
        "        f'{df_combined[\"TPSA\"].mean():.2f}',\n",
        "        f'{df_combined[\"SA Score\"].mean():.2f}',\n",
        "        f'{df_combined[\"SA Score\"].min():.2f}',\n",
        "        f'{avg_internal_distance:.3f}',\n",
        "        f'{df_combined[df_combined[\"FINAL RDKIT PRIORITY\"] == \"HIGH\"].shape[0]} / {len(df_combined)}',\n",
        "        f'{df_combined[df_combined[\"FINAL RDKIT PRIORITY\"] == \"HIGH\"].shape[0]} / {len(df_combined)}'\n",
        "    ]\n",
        "}\n",
        "df_summary = pd.DataFrame(summary_data)\n",
        "summary_markdown = df_summary.to_markdown(index=False)\n",
        "\n",
        "print(\"\\n\\n-----------------------------------------------\")\n",
        "print(\"TABLE 1: PROJECT SUMMARY AND PERFORMANCE METRICS\")\n",
        "print(\"-----------------------------------------------\\n\")\n",
        "display(Markdown(summary_markdown))\n",
        "\n",
        "# Full candidates table (Table 2)\n",
        "df_full_candidates = df_combined[[\n",
        "    'FINAL RDKIT PRIORITY',\n",
        "    'SMILES',\n",
        "    'MW',\n",
        "    'LogP',\n",
        "    'QED Score', # --- NEW COLUMN ADDED HERE ---\n",
        "    'TPSA',\n",
        "    'SA Score',\n",
        "    'Rot. Bonds',\n",
        "    'Fsp3',\n",
        "    'Ro5 Pass',\n",
        "    'Is PAINS',\n",
        "    'Tox Alert',\n",
        "    'Max Novelty Sim',\n",
        "]].drop(columns=['Sort_Novelty_Value', 'QED_Value'], errors='ignore')\n",
        "\n",
        "full_candidates_markdown = df_full_candidates.to_markdown(index=False)\n",
        "\n",
        "print(\"\\n\\n-----------------------------------------------\")\n",
        "print(f\"TABLE 2: FULL LEAD CANDIDATE MATRIX (Total: {len(df_combined)} Molecules)\")\n",
        "print(\"(Sorted by RDKIT Priority (HIGH first), then QED Score (Higher is Better))\")\n",
        "print(\"-----------------------------------------------\\n\")\n",
        "display(Markdown(full_candidates_markdown))\n",
        "\n",
        "# Final 2D structure grid image for leads\n",
        "if len(df_combined) > 0:\n",
        "    # Convert SMILES to molecules\n",
        "    grid_mols = [Chem.MolFromSmiles(smi) for smi in df_combined['SMILES'] if Chem.MolFromSmiles(smi)]\n",
        "\n",
        "    # Generate the 2D grid image\n",
        "    img_to_display = Draw.MolsToGridImage(grid_mols, molsPerRow=6, subImgSize=(300, 300))  # You can adjust `molsPerRow` as needed\n",
        "\n",
        "    # Save and display the final image with 600 DPI\n",
        "    img_filename = '/mnt/data/final_molecule_grid_600dpi.png'\n",
        "\n",
        "    try:\n",
        "        # Convert the RDKit image to a PIL image\n",
        "        img_to_display_pil = PILImage.fromarray(np.array(img_to_display))  # Convert to PIL image\n",
        "\n",
        "        # Save the image with 600 DPI resolution\n",
        "        img_to_display_pil.save(img_filename, dpi=(600, 600))  # Save with 600 DPI\n",
        "\n",
        "        # Display the saved image (optional)\n",
        "        display(PILImage.open(img_filename))  # Display the saved image\n",
        "        print(f\"Image successfully saved to {img_filename} with 600 DPI.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving image: {e}\")\n",
        "else:\n",
        "    print(\"No valid molecules for image generation.\")\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------------#\n",
        "#                      SMILE STRUCTURE GENERATION                                     #\n",
        "#-------------------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from PIL import Image as PILImage\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "# Function to generate individual 2D molecular structure images\n",
        "def save_molecules_as_images(smiles_list, output_dir='/content/Final_Images', image_format='png', dpi=600):\n",
        "    \"\"\"\n",
        "    Convert a list of SMILES strings to 2D structure images using RDKit and save them as PNG files.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Initialize a list to hold file paths for the images\n",
        "    image_paths = []\n",
        "\n",
        "    for i, smi in enumerate(smiles_list):\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            continue\n",
        "\n",
        "        # Generate 2D coordinates (force 2D structure)\n",
        "        Chem.rdDepictor.Compute2DCoords(mol)\n",
        "\n",
        "        # Create the image for the molecule\n",
        "        img = Draw.MolToImage(mol, size=(300, 300))\n",
        "\n",
        "        # Define output file path\n",
        "        output_file = os.path.join(output_dir, f\"molecule_{i + 1}.{image_format}\")\n",
        "        img.save(output_file, dpi=(dpi, dpi))  # Save with the specified DPI\n",
        "        image_paths.append(output_file)  # Add to the list\n",
        "\n",
        "        print(f\"Saved molecule {i + 1} to {output_file}\")\n",
        "\n",
        "    return image_paths\n",
        "\n",
        "# Function to create the molecule image matrix from individual images\n",
        "def create_molecule_matrix(image_paths, mols_per_row=6, tile_size=(300, 300)):\n",
        "    \"\"\"\n",
        "    Arrange molecule images into a matrix and save as a single PNG image.\n",
        "    \"\"\"\n",
        "    num_molecules = len(image_paths)\n",
        "    num_rows = math.ceil(num_molecules / mols_per_row)\n",
        "\n",
        "    # Create a blank white canvas for the matrix\n",
        "    matrix_width = mols_per_row * tile_size[0]\n",
        "    matrix_height = num_rows * tile_size[1]\n",
        "    matrix_img = PILImage.new('RGB', (matrix_width, matrix_height), (255, 255, 255))\n",
        "\n",
        "    # Paste each molecule image onto the matrix canvas\n",
        "    for idx, image_path in enumerate(image_paths):\n",
        "        row = idx // mols_per_row\n",
        "        col = idx % mols_per_row\n",
        "        img = PILImage.open(image_path)\n",
        "\n",
        "        # Resize the image to match the tile size (in case of differing sizes)\n",
        "        img = img.resize(tile_size)\n",
        "        matrix_img.paste(img, (col * tile_size[0], row * tile_size[1]))\n",
        "\n",
        "    # Save the final matrix image\n",
        "    matrix_output_file = '/content/Final_Images/molecule_matrix.png'\n",
        "    matrix_img.save(matrix_output_file, dpi=(600, 600))  # Save with 600 DPI\n",
        "    print(f\"Final molecule matrix saved to {matrix_output_file}\")\n",
        "\n",
        "    # Display the final image (optional)\n",
        "    display(matrix_img)\n",
        "\n",
        "    return matrix_output_file\n",
        "\n",
        "\n",
        "# Extract SMILES list from the df_combined DataFrame\n",
        "smiles_list = df_combined['SMILES'].tolist()\n",
        "\n",
        "# Step 1: Generate individual images for each molecule\n",
        "image_paths = save_molecules_as_images(smiles_list, output_dir='/content/Final_Images', image_format='png', dpi=600)\n",
        "\n",
        "# Step 2: Create the matrix of all generated images\n",
        "matrix_image_path = create_molecule_matrix(image_paths, mols_per_row=6, tile_size=(300, 300))\n",
        "\n",
        "print(f\"Matrix image created and saved at: {matrix_image_path}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "--enFj8jqDHP"
      },
      "id": "--enFj8jqDHP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}